#!/usr/bin/env python3
"""
Official SWE-bench Evaluation Runner

This script runs the official SWE-bench evaluation harness
on predictions generated by run_agent.py
"""

import os
import sys
import json
import subprocess
import argparse
from pathlib import Path
from datetime import datetime


def run_swebench_evaluation(
    predictions_path: str,
    dataset_name: str = "princeton-nlp/SWE-bench_Lite",
    split: str = "test",
    max_workers: int = 4,
    run_id: str = None,
    instance_ids: list = None,
    timeout: int = 1800,
):
    """Run official SWE-bench evaluation."""

    if run_id is None:
        run_id = f"sage_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    print(f"{'='*60}")
    print("SWE-bench Official Evaluation")
    print(f"{'='*60}")
    print(f"Predictions: {predictions_path}")
    print(f"Dataset: {dataset_name}")
    print(f"Run ID: {run_id}")
    print(f"Max Workers: {max_workers}")
    print()

    # Build command
    cmd = [
        sys.executable, "-m", "swebench.harness.run_evaluation",
        "--dataset_name", dataset_name,
        "--split", split,
        "--predictions_path", predictions_path,
        "--max_workers", str(max_workers),
        "--run_id", run_id,
        "--timeout", str(timeout),
    ]

    if instance_ids:
        cmd.extend(["--instance_ids"] + instance_ids)

    print(f"Running: {' '.join(cmd)}")
    print()

    # Run evaluation
    try:
        result = subprocess.run(
            cmd,
            capture_output=False,  # Show output in real-time
            text=True,
        )
        return result.returncode == 0
    except Exception as e:
        print(f"Error running evaluation: {e}")
        return False


def analyze_results(run_id: str, output_dir: str = None):
    """Analyze evaluation results."""
    if output_dir is None:
        output_dir = Path.cwd()
    else:
        output_dir = Path(output_dir)

    # Look for results file
    results_patterns = [
        output_dir / f"{run_id}.json",
        output_dir / "logs" / f"{run_id}" / "results.json",
        Path.home() / ".swebench" / "logs" / f"{run_id}" / "results.json",
    ]

    results_file = None
    for pattern in results_patterns:
        if pattern.exists():
            results_file = pattern
            break

    if not results_file:
        print(f"Could not find results file for run_id: {run_id}")
        return

    print(f"\n{'='*60}")
    print("EVALUATION RESULTS")
    print(f"{'='*60}")

    with open(results_file) as f:
        results = json.load(f)

    # Count results
    total = len(results)
    resolved = sum(1 for r in results.values() if r.get("resolved", False))

    print(f"Total instances: {total}")
    print(f"Resolved: {resolved}")
    print(f"Pass rate: {resolved/total*100:.1f}%")
    print()

    # Show per-instance results
    print("Per-instance results:")
    for instance_id, result in sorted(results.items()):
        status = "✅" if result.get("resolved", False) else "❌"
        print(f"  {status} {instance_id}")


def extract_existing_patches(test_dir: str, output_file: str = "predictions.json"):
    """Extract patches from existing test directories."""
    test_path = Path(test_dir)
    predictions = []

    for instance_dir in test_path.iterdir():
        if not instance_dir.is_dir():
            continue
        if instance_dir.name.startswith("."):
            continue

        instance_id = instance_dir.name

        # Check if it looks like an instance directory
        if "__" not in instance_id:
            continue

        print(f"Extracting patch from: {instance_id}")

        # Get git diff
        try:
            result = subprocess.run(
                ["git", "diff"],
                cwd=str(instance_dir),
                capture_output=True,
                text=True,
            )
            patch = result.stdout.strip()

            if patch:
                predictions.append({
                    "instance_id": instance_id,
                    "model_patch": patch,
                    "model_name_or_path": "sage-agent",
                })
                print(f"  ✅ Extracted patch ({len(patch)} chars)")
            else:
                print(f"  ⚠️ No changes found")

        except Exception as e:
            print(f"  ❌ Error: {e}")

    # Save predictions
    output_path = Path(output_file)
    with open(output_path, "w") as f:
        json.dump(predictions, f, indent=2)

    print(f"\nSaved {len(predictions)} predictions to: {output_path}")
    return predictions


def main():
    parser = argparse.ArgumentParser(description="Run official SWE-bench evaluation")
    subparsers = parser.add_subparsers(dest="command", help="Command to run")

    # Evaluate command
    eval_parser = subparsers.add_parser("evaluate", help="Run evaluation")
    eval_parser.add_argument(
        "predictions",
        help="Path to predictions.json file",
    )
    eval_parser.add_argument(
        "--dataset",
        default="princeton-nlp/SWE-bench_Lite",
        help="Dataset name",
    )
    eval_parser.add_argument(
        "--split",
        default="test",
        help="Dataset split",
    )
    eval_parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of parallel workers",
    )
    eval_parser.add_argument(
        "--run-id",
        help="Run ID for results",
    )
    eval_parser.add_argument(
        "--instances",
        nargs="+",
        help="Specific instance IDs to evaluate",
    )
    eval_parser.add_argument(
        "--timeout",
        type=int,
        default=1800,
        help="Timeout per instance in seconds",
    )

    # Extract command
    extract_parser = subparsers.add_parser("extract", help="Extract patches from test directories")
    extract_parser.add_argument(
        "test_dir",
        help="Directory containing test instance directories",
    )
    extract_parser.add_argument(
        "--output",
        default="predictions.json",
        help="Output predictions file",
    )

    # Results command
    results_parser = subparsers.add_parser("results", help="Analyze evaluation results")
    results_parser.add_argument(
        "run_id",
        help="Run ID to analyze",
    )
    results_parser.add_argument(
        "--dir",
        help="Directory containing results",
    )

    args = parser.parse_args()

    if args.command == "evaluate":
        run_swebench_evaluation(
            predictions_path=args.predictions,
            dataset_name=args.dataset,
            split=args.split,
            max_workers=args.workers,
            run_id=args.run_id,
            instance_ids=args.instances,
            timeout=args.timeout,
        )

    elif args.command == "extract":
        extract_existing_patches(
            test_dir=args.test_dir,
            output_file=args.output,
        )

    elif args.command == "results":
        analyze_results(
            run_id=args.run_id,
            output_dir=args.dir,
        )

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
